{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNmYKY3xINhOawedi2WQZnS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyasHidayatR/Python_SimpleProgram/blob/main/ChatbotAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Data"
      ],
      "metadata": {
        "id": "p-yE3JMWIhps"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-7wH9OL9I00",
        "outputId": "25f204ad-1a9b-43a5-b393-aca7c6af1d59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100/1000, cost: 1.0454\n",
            "Epoch 200/1000, cost: 0.4566\n",
            "Epoch 300/1000, cost: 0.2358\n",
            "Epoch 400/1000, cost: 0.1041\n",
            "Epoch 500/1000, cost: 0.0482\n",
            "Epoch 600/1000, cost: 0.0211\n",
            "Epoch 700/1000, cost: 0.0101\n",
            "Epoch 800/1000, cost: 0.0038\n",
            "Epoch 900/1000, cost: 0.0011\n",
            "Epoch 1000/1000, cost: 0.0006\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import json\n",
        "\n",
        "with open(\"intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "words = []\n",
        "labels = []\n",
        "docs_x = []\n",
        "docs_y = []\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        words.extend(wrds)\n",
        "        docs_x.append(wrds)\n",
        "        docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "        labels.append(intent[\"tag\"])\n",
        "\n",
        "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "words = sorted(list(set(words)))\n",
        "labels = sorted(labels)\n",
        "\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "output_empty = [0] * len(labels)\n",
        "\n",
        "for x, doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "    wrds = [stemmer.stem(w) for w in doc]\n",
        "    for w in words:\n",
        "        if w in wrds:\n",
        "            bag.append(1)\n",
        "        else:\n",
        "            bag.append(0)\n",
        "\n",
        "    output_row = list(output_empty)\n",
        "    output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "training = np.array(training)\n",
        "output = np.array(output)\n",
        "\n",
        "with open(\"data.json\", \"w\") as file:\n",
        "    json.dump({\"training\": training.tolist(), \"output\": output.tolist()}, file, indent=4)\n",
        "\n",
        "# Define the TensorFlow model\n",
        "input_size = len(training[0])\n",
        "output_size = len(output[0])\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, input_size])\n",
        "Y = tf.placeholder(tf.float32, [None, output_size])\n",
        "\n",
        "hidden_size = 8\n",
        "\n",
        "weights = {\n",
        "    'hidden': tf.Variable(tf.random_normal([input_size, hidden_size])),\n",
        "    'output': tf.Variable(tf.random_normal([hidden_size, output_size]))\n",
        "}\n",
        "biases = {\n",
        "    'hidden': tf.Variable(tf.random_normal([hidden_size])),\n",
        "    'output': tf.Variable(tf.random_normal([output_size]))\n",
        "}\n",
        "\n",
        "hidden_layer = tf.add(tf.matmul(X, weights['hidden']), biases['hidden'])\n",
        "hidden_layer = tf.nn.relu(hidden_layer)\n",
        "\n",
        "output_layer = tf.matmul(hidden_layer, weights['output']) + biases['output']\n",
        "\n",
        "# Define the loss and optimizer\n",
        "# Define the loss and optimizer\n",
        "with tf.name_scope('optimizer'):\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=Y))\n",
        "    optimizer = tf.train.AdamOptimizer(name='Adam-Optimizer').minimize(cost)\n",
        "\n",
        "# Initialize variables and create a session\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 1000\n",
        "    batch_size = 8\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, len(training), batch_size):\n",
        "            batch_x = training[i:i + batch_size]\n",
        "            batch_y = output[i:i + batch_size]\n",
        "            sess.run(optimizer, feed_dict={X: batch_x, Y: batch_y})\n",
        "\n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            c = sess.run(cost, feed_dict={X: training, Y: output})\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, cost: {c:.4f}\")\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "\n",
        "    # Save the model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(sess, \"model1.tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi menguji chatbot"
      ],
      "metadata": {
        "id": "ioKZumOKIoS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot(sess, words, labels, input_text):\n",
        "    input_text = input_text.lower()\n",
        "    input_words = nltk.word_tokenize(input_text)\n",
        "    input_words = [stemmer.stem(word) for word in input_words]\n",
        "\n",
        "    input_bag = [0] * len(words)\n",
        "    for word in input_words:\n",
        "        if word in words:\n",
        "            input_bag[words.index(word)] = 1\n",
        "\n",
        "    input_bag = np.array(input_bag)\n",
        "    input_bag = input_bag.reshape(1, -1)  # Reshape to a 2D array\n",
        "\n",
        "    results = sess.run(output_layer, feed_dict={X: input_bag})\n",
        "    results_index = np.argmax(results)\n",
        "    tag = labels[results_index]\n",
        "\n",
        "    for intent in data[\"intents\"]:\n",
        "        if intent['tag'] == tag:\n",
        "            responses = intent['responses']\n",
        "\n",
        "    return random.choice(responses)"
      ],
      "metadata": {
        "id": "0amcRiVU9qzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# Load the saved model\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('model1.tf.meta')\n",
        "    saver.restore(sess, \"model1.tf\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == \"quit\":\n",
        "            break\n",
        "        response = chat_with_bot(sess, words, labels, user_input)\n",
        "        print(\"Bot:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_DjJHR19uUw",
        "outputId": "c7eb5995-f76b-45c7-c87a-42ad8467ded3"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Hai\n",
            "Bot: Halo! Terima kasih, semoga juga untukmu.\n",
            "You: quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fungsi uji chatbot dalam bentuk API"
      ],
      "metadata": {
        "id": "prKPN40qIvL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import random\n",
        "\n",
        "# Import semua yang diperlukan, termasuk model dan fungsi chat_with_bot\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load the saved model\n",
        "with tf.Session() as sess:\n",
        "    saver = tf.train.import_meta_graph('model1.tf.meta')\n",
        "    saver.restore(sess, \"model1.tf\")\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_input = request.json.get('user_input')\n",
        "    if user_input.lower() == \"quit\":\n",
        "        response = \"Bot: Goodbye!\"\n",
        "    else:\n",
        "        response = chat_with_bot(sess, words, labels, user_input)\n",
        "    return jsonify({'response': response})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRponmkA-9la",
        "outputId": "f58abcba-cdda-4207-c9e2-0e9e374ba7bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    }
  ]
}